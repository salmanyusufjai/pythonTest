import csv

# Define dummy data for each table
data = {
    "scorecheck": [
        ["entitykey", "companynumber", "score", "grade", "processingdate"],
        ["e1", "c1", 85.5, "A", "01/03/2024"],
        ["e2", "c2", 75.0, "B", "01/03/2024"],
        ["e3", "c3", 65.0, "C", "01/03/2024"]
    ],
    "acl": [
        ["entitykey", "companynumber", "finallimit", "processingdate"],
        ["e1", "c1", 10000.0, "01/03/2024"],
        ["e2", "c2", 20000.0, "01/03/2024"],
        ["e3", "c3", 15000.0, "01/03/2024"]
    ],
    "noliacl": [
        ["entitykey", "companynumber", "finallimit", "grade", "score", "processingdate"],
        ["e1", "c1", 5000.0, "A", 85.5, "01/03/2024"],
        ["e2", "c2", 7000.0, "B", 75.0, "01/03/2024"],
        ["e3", "c3", 6000.0, "C", 65.0, "01/03/2024"]
    ],
    "protectscore": [
        ["entitykey", "companynumber", "protectcode", "protectscore", "processingdate"],
        ["e1", "c1", "PC1", 95.5, "01/03/2024"],
        ["e2", "c2", "PC2", 85.0, "01/03/2024"],
        ["e3", "c3", "PC3", 75.0, "01/03/2024"]
    ]
}

# Save dummy data to CSV files
for table_name, rows in data.items():
    with open(f"{table_name}.csv", "w", newline='') as file:
        writer = csv.writer(file)
        writer.writerows(rows)


-----------


from google.cloud import storage

# Initialize Google Cloud Storage client
storage_client = storage.Client()

# Define your GCS bucket name
bucket_name = 'your_bucket_name'  # Replace with your bucket name

# Upload CSV files to GCS
bucket = storage_client.bucket(bucket_name)

for table_name in data.keys():
    blob = bucket.blob(f"{table_name}.csv")
    blob.upload_from_filename(f"{table_name}.csv")
    print(f"Uploaded {table_name}.csv to gs://{bucket_name}/{table_name}.csv")


------
from google.cloud import bigquery

# Initialize BigQuery client
client = bigquery.Client()

# Define the dataset ID
dataset_id = 'your_dataset_id'  # Replace with your dataset ID

# Define partitioning configuration
partitioning = bigquery.TimePartitioning(
    type_=bigquery.TimePartitioningType.DAY,
    field="_PARTITIONTIME",
)

# Define table schemas
schemas = {
    "scorecheck": [
        bigquery.SchemaField("entitykey", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("companynumber", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("score", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("grade", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("processingdate", "STRING", mode="REQUIRED")
    ],
    "acl": [
        bigquery.SchemaField("entitykey", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("companynumber", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("finallimit", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("processingdate", "STRING", mode="REQUIRED")
    ],
    "noliacl": [
        bigquery.SchemaField("entitykey", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("companynumber", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("finallimit", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("grade", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("score", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("processingdate", "STRING", mode="REQUIRED")
    ],
    "protectscore": [
        bigquery.SchemaField("entitykey", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("companynumber", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("protectcode", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("protectscore", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("processingdate", "STRING", mode="REQUIRED")
    ]
}

# Function to create a partitioned table
def create_partitioned_table(table_id, schema):
    table = bigquery.Table(table_id, schema=schema)
    table.time_partitioning = partitioning
    table = client.create_table(table)  # API request
    print(f"Created table {table.project}.{table.dataset_id}.{table.table_id}")

# Create each table
for table_name, schema in schemas.items():
    table_id = f"{client.project}.{dataset_id}.{table_name}"
    create_partitioned_table(table_id, schema)

# Function to load CSV data into a specific partition of a BigQuery table
def load_csv_to_bq(table_name):
    table_id = f"{client.project}.{dataset_id}.{table_name}"
    gcs_uri = f"gs://{bucket_name}/{table_name}.csv"
    
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        autodetect=True,
        time_partitioning=bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field="_PARTITIONTIME"
        ),
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
        # Provide a partition field value in the schema to insert the records into a specific partition
        schema_update_options=[bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]
    )
    
    # Specify the partition decorator
    load_job = client.load_table_from_uri(
        gcs_uri,
        f"{table_id}$20240301",  # Load data into the partition for 1st March 2024
        job_config=job_config
    )

    load_job.result()  # Wait for the job to complete
    print(f"Loaded {table_name}.csv into {table_id}$20240301")

# Load CSV data into each table
for table_name in schemas.keys():
    load_csv_to_bq(table_name)
