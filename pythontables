from google.cloud import bigquery
from google.cloud.bigquery import TimePartitioning, PartitionRange

# Initialize BigQuery client
client = bigquery.Client()

# Define the dataset ID
dataset_id = 'your_dataset_id'  # Replace with your dataset ID

# Define partitioning configuration for March 1, 2024
partitioning = TimePartitioning(
    type_=bigquery.TimePartitioningType.DAY,
    field="_PARTITIONTIME",
    expiration_ms=7776000000,  # Optional: partition expiration (in milliseconds)
    require_partition_filter=True  # Optional: require partition filter
)

# Define table schemas and create table configurations
tables = {
    "scorecheck": [
        bigquery.SchemaField("entitykey", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("companynumber", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("score", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("grade", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("processingdate", "STRING", mode="REQUIRED")
    ],
    "acl": [
        bigquery.SchemaField("entitykey", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("companynumber", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("finallimit", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("processingdate", "STRING", mode="REQUIRED")
    ],
    "noliacl": [
        bigquery.SchemaField("entitykey", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("companynumber", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("finallimit", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("grade", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("score", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("processingdate", "STRING", mode="REQUIRED")
    ],
    "protectscore": [
        bigquery.SchemaField("entitykey", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("companynumber", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("protectcode", "STRING", mode="REQUIRED"),
        bigquery.SchemaField("protectscore", "FLOAT", mode="REQUIRED"),
        bigquery.SchemaField("processingdate", "STRING", mode="REQUIRED")
    ]
}

# Function to create a partitioned table
def create_partitioned_table(table_id, schema):
    table = bigquery.Table(table_id, schema=schema)
    table.time_partitioning = partitioning
    table = client.create_table(table)  # API request
    print(f"Created table {table.project}.{table.dataset_id}.{table.table_id}")

# Create each table
for table_name, schema in tables.items():
    table_id = f"{client.project}.{dataset_id}.{table_name}"
    create_partitioned_table(table_id, schema)


----------------------------------------

import csv

# Define dummy data for each table
data = {
    "scorecheck": [
        ["entitykey", "companynumber", "score", "grade", "processingdate"],
        ["e1", "c1", 85.5, "A", "01/03/2024"],
        ["e2", "c2", 75.0, "B", "01/03/2024"],
        ["e3", "c3", 65.0, "C", "01/03/2024"]
    ],
    "acl": [
        ["entitykey", "companynumber", "finallimit", "processingdate"],
        ["e1", "c1", 10000.0, "01/03/2024"],
        ["e2", "c2", 20000.0, "01/03/2024"],
        ["e3", "c3", 15000.0, "01/03/2024"]
    ],
    "noliacl": [
        ["entitykey", "companynumber", "finallimit", "grade", "score", "processingdate"],
        ["e1", "c1", 5000.0, "A", 85.5, "01/03/2024"],
        ["e2", "c2", 7000.0, "B", 75.0, "01/03/2024"],
        ["e3", "c3", 6000.0, "C", 65.0, "01/03/2024"]
    ],
    "protectscore": [
        ["entitykey", "companynumber", "protectcode", "protectscore", "processingdate"],
        ["e1", "c1", "PC1", 95.5, "01/03/2024"],
        ["e2", "c2", "PC2", 85.0, "01/03/2024"],
        ["e3", "c3", "PC3", 75.0, "01/03/2024"]
    ]
}

# Save dummy data to CSV files
for table_name, rows in data.items():
    with open(f"{table_name}.csv", "w", newline='') as file:
        writer = csv.writer(file)
        writer.writerows(rows)

---------------------------------------


from google.cloud import storage

# Initialize Google Cloud Storage client
storage_client = storage.Client()

# Define your GCS bucket name
bucket_name = 'your_bucket_name'  # Replace with your bucket name

# Upload CSV files to GCS
bucket = storage_client.bucket(bucket_name)

for table_name in data.keys():
    blob = bucket.blob(f"{table_name}.csv")
    blob.upload_from_filename(f"{table_name}.csv")
    print(f"Uploaded {table_name}.csv to gs://{bucket_name}/{table_name}.csv")
------------------


from google.cloud import bigquery

# Initialize BigQuery client
client = bigquery.Client()

# Define the dataset ID
dataset_id = 'your_dataset_id'  # Replace with your dataset ID

# Function to load CSV data into a BigQuery table
def load_csv_to_bq(table_name):
    table_id = f"{client.project}.{dataset_id}.{table_name}"
    gcs_uri = f"gs://{bucket_name}/{table_name}.csv"
    
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        autodetect=True,
        time_partitioning=bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.DAY,
            field="_PARTITIONTIME"
        )
    )
    
    load_job = client.load_table_from_uri(
        gcs_uri,
        table_id,
        job_config=job_config
    )

    load_job.result()  # Wait for the job to complete
    print(f"Loaded {table_name}.csv into {table_id}")

# Load CSV data into each table
for table_name in data.keys():
    load_csv_to_bq(table_name)
