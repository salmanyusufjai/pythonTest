from google.cloud import storage
import fastavro
import sys

def get_size(obj, seen=None):
    if seen is None:
        seen = set()
    size = 0
    if id(obj) in seen:
        return size
    seen.add(id(obj))
    if isinstance(obj, (int, float, bool, str, bytes)):
        size += sys.getsizeof(obj)
    elif isinstance(obj, (list, tuple, set, frozenset)):
        size += sys.getsizeof(obj)
        size += sum(get_size(item, seen) for item in obj)
    elif isinstance(obj, dict):
        size += sys.getsizeof(obj)
        size += sum(get_size(k, seen) + get_size(v, seen) for k, v in obj.items())
    elif hasattr(obj, '__dict__'):
        size += get_size(obj.__dict__, seen)
    elif hasattr(obj, '__slots__'):
        size += sum(get_size(getattr(obj, slot), seen) for slot in obj.__slots__)
    return size

def read_avro_from_gcs(bucket_name, avro_blob_name):
    # Initialize a GCS client
    storage_client = storage.Client()

    # Get the bucket object
    bucket = storage_client.get_bucket(bucket_name)

    # Get the blob (Avro file) from the bucket
    blob = bucket.blob(avro_blob_name)

    # Download the Avro file to a local temporary file
    temp_file = "/tmp/temp_avro_file.avro"  # You can change the path as needed
    blob.download_to_filename(temp_file)

    # Open and read the Avro file
    with open(temp_file, 'rb') as avro_file:
        reader = fastavro.reader(avro_file)
        for row in reader:
            row_size = get_size(row)
            if row_size > 500 * 1024 * 1024:  # 500MB in bytes
                print(f"Row Size: {row_size} bytes")
                print(row)  # Print the entire row

if __name__ == "__main__":
    # Replace 'your-bucket-name' with the name of your GCS bucket
    bucket_name = "your-bucket-name"
    
    # Replace 'your-avro-blob-name' with the name of the Avro blob in the bucket
    avro_blob_name = "your-avro-blob-name"
    
    read_avro_from_gcs(bucket_name, avro_blob_name)
